<html><head><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous" rel="stylesheet"></link><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous" rel="stylesheet"></link><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} </style></head><body><div class="page-header"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPCUBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPCRHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86RHEL7</a></li></ul></div><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active">Packages</a><a href="#" id="anchor_accumulo" onclick="showme(this.id);" class="list-group-item list-group-item-action">ACCUMULO</a><a href="#" id="anchor_ambari" onclick="showme(this.id);" class="list-group-item list-group-item-action">AMBARI</a><a href="#" id="anchor_atlas" onclick="showme(this.id);" class="list-group-item list-group-item-action">ATLAS</a><a href="#" id="anchor_falcon" onclick="showme(this.id);" class="list-group-item list-group-item-action">FALCON</a><a href="#" id="anchor_flume" onclick="showme(this.id);" class="list-group-item list-group-item-action">FLUME</a><a href="#" id="anchor_hbase" onclick="showme(this.id);" class="list-group-item list-group-item-action">HBASE</a><a href="#" id="anchor_hive" onclick="showme(this.id);" class="list-group-item list-group-item-action">HIVE</a><a href="#" id="anchor_kafka" onclick="showme(this.id);" class="list-group-item list-group-item-action">KAFKA</a><a href="#" id="anchor_knox" onclick="showme(this.id);" class="list-group-item list-group-item-action">KNOX</a><a href="#" id="anchor_metron" onclick="showme(this.id);" class="list-group-item list-group-item-action">METRON</a><a href="#" id="anchor_oozie" onclick="showme(this.id);" class="list-group-item list-group-item-action">OOZIE</a><a href="#" id="anchor_phoenix" onclick="showme(this.id);" class="list-group-item list-group-item-action">PHOENIX</a><a href="#" id="anchor_pig" onclick="showme(this.id);" class="list-group-item list-group-item-action">PIG</a><a href="#" id="anchor_ranger" onclick="showme(this.id);" class="list-group-item list-group-item-action">RANGER</a><a href="#" id="anchor_slider" onclick="showme(this.id);" class="list-group-item list-group-item-action">SLIDER</a><a href="#" id="anchor_spark" onclick="showme(this.id);" class="list-group-item list-group-item-action">SPARK</a><a href="#" id="anchor_sqoop" onclick="showme(this.id);" class="list-group-item list-group-item-action">SQOOP</a><a href="#" id="anchor_storm" onclick="showme(this.id);" class="list-group-item list-group-item-action">STORM</a><a href="#" id="anchor_tez" onclick="showme(this.id);" class="list-group-item list-group-item-action">TEZ</a><a href="#" id="anchor_zeppelin" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZEPPELIN</a><a href="#" id="anchor_zookeeper" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZOOKEEPER</a></div></div><div style="display: table-cell"><div style="display:none;" class="panel panel-default" name="data" id="accumulo"><div class="panel-heading">ACCUMULO</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: cdd8c9ee35c47963fa91bb702c6a4465237146c5</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1697</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1697</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1697</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1697</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol><div><li>test timed out after 60000 milliseconds</li></div></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="ambari"><div class="panel-heading">AMBARI</div><div class="panel-body"><div>Branch Details: origin/trunk</div><div>Last Revision: 9f6612cb190196ed8d646781bb3d5e0c836fe9d2</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5502</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5502</div><div>Failed Count : 14</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5502</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img>ABORTED</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWithConfigsUpdate</li></div></ol></td><td><ol><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntityTypes</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testDeleteEntities</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div><div><li>org.apache.ambari.server.state.cluster.ServiceComponentHostConcurrentWriteDeadlockTest.testConcurrentWriteDeadlock</li></div></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>expected:&lt;400&gt; but was:&lt;244&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=54 WAITING on org.eclipse.persistence.internal.identitymaps.HardCacheWeakIdentityMap$ReferenceCacheKey@62cd120a owned by "Thread-31" Id=63
 at java.lang.Object.wait(Native Method)
 -  waiting on org.eclipse.persistence.internal.identitymaps.HardCacheWeakIdentityMap$ReferenceCacheKey@62cd120a
 at java.lang.Object.wait(Object.java:502)
 at org.ec</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-41" Id=75 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@736c178d
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@736c178d
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAnd</li></div></ol></td><td><ol><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2636322571273065225.8: libsnappy.so.1: cannot open shared object file: No such file or directory]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=53 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@4c3b4f9c owned by "Thread-35" Id=66
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@4c3b4f9c
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.Abstrac</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-4" Id=26 BLOCKED on org.h2.engine.Database@494da667 owned by "Thread-6" Id=28
 at org.h2.command.Command.executeUpdate(Command.java:252)
 -  blocked on org.h2.engine.Database@494da667
 at org.h2.jdbc.JdbcPreparedStatement.executeUpdateInternal(JdbcPreparedStatement.java:160)
 at org.h2.jdbc.JdbcPreparedStatement.executeUpdate(JdbcPreparedStatement.jav</li></div></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</div></li><li><div>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</div></li><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWithConfigsUpdate</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testGetEntityTypes</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testDeleteEntities</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</div></li><li><div>org.apache.ambari.server.state.cluster.ServiceComponentHostConcurrentWriteDeadlockTest.testConcurrentWriteDeadlock</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="atlas"><div class="panel-heading">ATLAS</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 6770091aa172576d398e4980f0dacbea65f733ea</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="falcon"><div class="panel-heading">FALCON</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 1a5b4f6a509187498a267b0e375c6a065f947af5</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1006</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1006</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 16</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 12</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div></ol></td><td><ol><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div><div><li>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</li></div></ol></td><td><ol><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkProcess</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkSQLProcess</li></div></ol></td><td><ol><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkProcess</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkSQLProcess</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div></ol></td><td><ol><div><li>Unable to restore configurations for entity type PROCESS</li></div><div><li>expected [1] but found [null]</li></div></ol></td><td><ol><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div></ol></td><td><ol><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li><li><div>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="flume"><div class="panel-heading">FLUME</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/trunk</div><div>Last Revision: d1f24f56ce9714bb3e1edc671da290c75a17dead</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1181</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1207</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1180</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1207</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="hbase"><div class="panel-heading">HBASE</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: a03d09abd72789bbf9364d8a9b2c54d0e9351af9</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4510</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 4522</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 4512</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 4522</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.hadoop.hbase.client.TestScannersFromClientSide.testAsyncScannerWithSmallData</li></div></ol></td><td><ol><div><li>org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters</li></div></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup(TestEnableRSGroup.java:80)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Time-limited test-SendThread(localhost:58765)</li></div></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hbase.client.TestScannersFromClientSide.testAsyncScannerWithSmallData</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithTwoDifferentZKClusters</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="hive"><div class="panel-heading">HIVE</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: fffbec0650affbe9e78485e51de5b8f9d7b293a5</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6852</div><div>Failed Count : 13</div><div>Skipped Count : 0</div></td><td><div>Total Count : 6852</div><div>Failed Count : 12</div><div>Skipped Count : 0</div></td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 6852</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img>ABORTED</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol></ol></td><td><ol><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol></ol></td><td><ol><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li><li><div>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</div></li></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="kafka"><div class="panel-heading">KAFKA</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/trunk</div><div>Last Revision: c5ba0da99301c9ffc1216e23541cbe7d2cc9a102</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8743</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 8743</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 6855</div><div>Failed Count : 117</div><div>Skipped Count : 0</div></td><td><div>Total Count : 8743</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</li></div><div><li>kafka.api.UserQuotaTest.testQuotaOverrideDelete</li></div></ol></td><td><ol><div><li>kafka.integration.MetricsDuringTopicCreationDeletionTest.testMetricsDuringTopicCreateDelete</li></div></ol></td><td><ol><div><li>org.apache.kafka.clients.producer.internals.ProducerBatchTest.testSplitPreservesMagicAndCompressionType</li></div><div><li>org.apache.kafka.clients.producer.internals.ProducerBatchTest.testSplitPreservesHeaders</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=1, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=1, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=1, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=1, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=2, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationV2[magic=2, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=2, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=2, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=2, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonControlBatch[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertV2ToV1UsingMixedCreateAndLogAppendTime[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnBuildWhenAborted[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingCreateTime[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidEpoch[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV0[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV1[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonTransactionalBatch[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingLogAppendTime[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalRecordSet[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalWithInvalidPID[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendedChecksumConsistency[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidBaseSequence[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertToV1WithMixedV0AndV2Data[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEmptyRecordSet[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testEstimatedSizeInBytes[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnCloseWhenAborted[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnAppendWhenAborted[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testBuffersDereferencedOnClose[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testSmallWriteLimit[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldResetBufferToInitialPositionOnAbort[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.writePastLimit[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendAtInvalidOffset[bufferOffset=0, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonControlBatch[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertV2ToV1UsingMixedCreateAndLogAppendTime[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnBuildWhenAborted[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingCreateTime[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidEpoch[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV0[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV1[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonTransactionalBatch[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingLogAppendTime[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalRecordSet[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalWithInvalidPID[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendedChecksumConsistency[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidBaseSequence[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertToV1WithMixedV0AndV2Data[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEmptyRecordSet[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testEstimatedSizeInBytes[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnCloseWhenAborted[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnAppendWhenAborted[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testBuffersDereferencedOnClose[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testSmallWriteLimit[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldResetBufferToInitialPositionOnAbort[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.writePastLimit[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendAtInvalidOffset[bufferOffset=15, compression=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[2 magic=0, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[6 magic=1, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToEmptyBatchRetention[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesPartitionLeaderEpoch[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethodWithHeaders[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesProducerInfo[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[10 magic=2, firstOffset=0, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[14 magic=0, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[18 magic=1, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToEmptyBatchRetention[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testToString[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesPartitionLeaderEpoch[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethodWithHeaders[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesProducerInfo[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div><div><li>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[22 magic=2, firstOffset=57, compressionType=SNAPPY]</li></div></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>kafka.admin.AdminOperationException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/topics/test</li></div><div><li>java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.</li></div></ol></td><td><ol><div><li>org.scalatest.junit.JUnitTestFailedError: 1 did not equal 0 UnderReplicatedPartitionCount not 0: 1</li></div></ol></td><td><ol><div><li>org.apache.kafka.common.KafkaException: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.1.7-f83d761e-2bcc-4207-ae5d-f489ab85f7e1-libsnappyjava.so: /lib64/ld64.so.2: version `GLIBC_2.22' not found (required by /tmp/snappy-1.1.7-f83d761e-2bcc-4207-ae5d-f489ab85f7e1-libsnappyjava.so)</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>java.lang.Exception: Unexpected exception, expected&lt;java.lang.IllegalArgumentException&gt; but was&lt;org.apache.kafka.common.KafkaException&gt;</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div><div><li>org.apache.kafka.common.KafkaException: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy</li></div></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</div></li><li><div>kafka.api.UserQuotaTest.testQuotaOverrideDelete</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>kafka.integration.MetricsDuringTopicCreationDeletionTest.testMetricsDuringTopicCreateDelete</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.kafka.clients.producer.internals.ProducerBatchTest.testSplitPreservesMagicAndCompressionType</div></li><li><div>org.apache.kafka.clients.producer.internals.ProducerBatchTest.testSplitPreservesHeaders</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=1, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=1, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=1, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=1, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testSimpleBatchIteration[magic=2, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationV2[magic=2, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationWithMultipleRecordsPerBatch[magic=2, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testBatchIterationIncompleteBatch[magic=2, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.FileLogInputStreamTest.testWriteTo[magic=2, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonControlBatch[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertV2ToV1UsingMixedCreateAndLogAppendTime[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnBuildWhenAborted[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingCreateTime[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidEpoch[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV0[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV1[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonTransactionalBatch[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingLogAppendTime[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalRecordSet[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalWithInvalidPID[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendedChecksumConsistency[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidBaseSequence[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertToV1WithMixedV0AndV2Data[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEmptyRecordSet[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testEstimatedSizeInBytes[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnCloseWhenAborted[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnAppendWhenAborted[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testBuffersDereferencedOnClose[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testSmallWriteLimit[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldResetBufferToInitialPositionOnAbort[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.writePastLimit[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendAtInvalidOffset[bufferOffset=0, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonControlBatch[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertV2ToV1UsingMixedCreateAndLogAppendTime[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnBuildWhenAborted[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingCreateTime[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidEpoch[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV0[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testCompressionRateV1[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEndTxnMarkerNonTransactionalBatch[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.buildUsingLogAppendTime[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalRecordSet[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteTransactionalWithInvalidPID[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendedChecksumConsistency[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteIdempotentWithInvalidBaseSequence[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.convertToV1WithMixedV0AndV2Data[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testWriteEmptyRecordSet[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testEstimatedSizeInBytes[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnCloseWhenAborted[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldThrowIllegalStateExceptionOnAppendWhenAborted[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testBuffersDereferencedOnClose[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testSmallWriteLimit[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.shouldResetBufferToInitialPositionOnAbort[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.writePastLimit[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsBuilderTest.testAppendAtInvalidOffset[bufferOffset=15, compression=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[2 magic=0, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[6 magic=1, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToEmptyBatchRetention[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesPartitionLeaderEpoch[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethodWithHeaders[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesProducerInfo[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[10 magic=2, firstOffset=0, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[14 magic=0, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[18 magic=1, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToAlreadyCompactedLog[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToEmptyBatchRetention[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToBatchDiscard[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testToString[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethod[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesLogAppendTime[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesPartitionLeaderEpoch[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterTo[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToWithUndersizedBuffer[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testHasRoomForMethodWithHeaders[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testFilterToPreservesProducerInfo[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li><li><div>org.apache.kafka.common.record.MemoryRecordsTest.testIterator[22 magic=2, firstOffset=57, compressionType=SNAPPY]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="knox"><div class="panel-heading">KNOX</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 21ac567db07c3f2196dfb41cb7bf9bbb786801a2</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="metron"><div class="panel-heading">METRON</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 03a4aa3846773065d51086167a28fb4c37d7b2a9</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1622</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1622</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1622</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1622</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="oozie"><div class="panel-heading">OOZIE</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 9e662c7350d64ddfa41e62882ce3328216f29dff</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2092</div><div>Failed Count : 33</div><div>Skipped Count : 0</div></td><td><div>Total Count : 2092</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 2092</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 2092</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</li></div></ol></td><td><ol><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.command.coord.TestCoordMaterializeTransitionXCommand.testLastOnlyMaterialization</li></div></ol></td><td><ol><div><li>org.apache.oozie.command.coord.TestCoordPushDependencyCheckXCommand.testTimeOutWithException2</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobsSortBy</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMemoryUsageAndSpeedOverflowToDisk</li></div></ol></td><td><ol><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>java.util.concurrent.ExecutionException: java.lang.NullPointerException</li></div><div><li>Render and write PNG failed for graph-workflow-simple.xml: java.util.concurrent.TimeoutException</li></div><div><li>Render and write SVG failed: java.util.concurrent.TimeoutException</li></div><div><li>java.util.concurrent.TimeoutException</li></div></ol></td><td><ol><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>expected:&lt;WAITING&gt; but was:&lt;READY&gt;</li></div></ol></td><td><ol><div><li>expected:&lt;TIMEDOUT&gt; but was:&lt;WAITING&gt;</li></div><div><li>expected:&lt;[PREP]&gt; but was:&lt;[RUNNING]&gt;</li></div><div><li>hcat://hcat.server.com:5080/mydb/mytbl/id=31662 is missing in cache</li></div></ol></td><td><ol><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</div></li><li><div>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.command.coord.TestCoordMaterializeTransitionXCommand.testLastOnlyMaterialization</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.command.coord.TestCoordPushDependencyCheckXCommand.testTimeOutWithException2</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobsSortBy</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMemoryUsageAndSpeedOverflowToDisk</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="phoenix"><div class="panel-heading">PHOENIX</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 916186e974da77e4b58ca9392c3052957b8eecba</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1658</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1658</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1658</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1658</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="pig"><div class="panel-heading">PIG</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/trunk</div><div>Last Revision: 1fcd7196e21117eb4c365f26adc19210f63fbdec</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 723</div><div>Failed Count : 6</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:41477/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-6568137530095803952.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-3064220285208467269.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output-9043849104151878999.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output-5311082049829581957.txt_1 does not exist.</li></div></ol></td><td><ol><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.pig.test.TestBuiltin.testSFPig</div></li><li><div>org.apache.pig.test.TestBuiltin.testUniqueID</div></li><li><div>org.apache.pig.test.TestLoad.testCommaSeparatedString5</div></li><li><div>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</div></li><li><div>org.apache.pig.test.TestSchema.testSchemaSerialization</div></li><li><div>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</div></li><li><div>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</div></li><li><div>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</div></li><li><div>org.apache.pig.test.TestStore.testBinStorageGetSchema</div></li><li><div>org.apache.pig.test.TestStore.testStoreComplexData</div></li><li><div>org.apache.pig.test.TestStore.testSetStoreSchema</div></li><li><div>org.apache.pig.test.TestStore.testSuccessFileCreation1</div></li><li><div>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</div></li><li><div>org.apache.pig.test.TestStore.testEmptyPartFileCreation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="ranger"><div class="panel-heading">RANGER</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 809a78b4a9549ac1d28ae600fc23e769c50ede20</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1033</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.hadoop.crypto.key.kms.server.RangerMasterKeyTest.testRangerMasterKey</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>Failure expected on retrieving a key with the wrong password</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.crypto.key.kms.server.RangerMasterKeyTest.testRangerMasterKey</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="slider"><div class="panel-heading">SLIDER</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/develop</div><div>Last Revision: 253371f43677421591e99429d03dd7feaa6468ee</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 0</div></td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 0</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1520543513122_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://9c5c4abe63f4:37523/cluster/app/application_1520543064865_0001; Host Details : local host is: "localhost"; destination host is: "http://9c5c4abe63f4:37523/cluster/app/application_1520543064865_0001":37523; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1520543457647 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://9c</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   46563        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://9c5c4abe63f4:46563/cluster/app/application_1520543314266_0001</li></div><div><li>Application not running: application_1520543337671_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1520543701065_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://aa81274aacdf:35545/cluster/app/application_1520543731971_0001; Host Details : local host is: "localhost"; destination host is: "http://aa81274aacdf:35545/cluster/app/application_1520543731971_0001":35545; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1520543355131 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://aa</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   37182        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://aa81274aacdf:37182/cluster/app/application_1520543330414_0001</li></div><div><li>Application not running: application_1520543406727_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="spark"><div class="panel-heading">SPARK</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: ac76eff6a88f6358a321b84cb5e60fb9d6403419</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 13778</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 15452</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 9425</div><div>Failed Count : 441</div><div>Skipped Count : 0</div></td><td><div>Total Count : 15452</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol><div><li>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.restarts</li></div><div><li>org.apache.spark.sql.streaming.continuous.ContinuousSuite.repeatedly restart</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy</li></div><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.sql.CachedTableSuite.SPARK-19765: UNCACHE TABLE should un-cache all cached plans that refer to this table</li></div><div><li>org.apache.spark.sql.CachedTableSuite.refreshByPath should refresh all cached plans with the specified path</li></div><div><li>org.apache.spark.sql.DataFrameJoinSuite.broadcast join hint using broadcast function</li></div><div><li>org.apache.spark.sql.DataFrameSuite.inputFiles</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-6941: Better error message for inserting into RDD-based Table</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-11301: fix case sensitivity for filter on partitioned columns</li></div><div><li>org.apache.spark.sql.DataFrameSuite.fix case sensitivity of partition by</li></div><div><li>org.apache.spark.sql.DatasetSuite.SPARK-22472: add null check for top-level primitive values</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-23072 Write and read back unicode column names - parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-22146 read files containing special characters using parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.Enabling/disabling ignoreMissingFiles using parquet</li></div><div><li>org.apache.spark.sql.MathFunctionsSuite.round/bround with table columns</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16337 temporary view refresh</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.case sensitivity support in temporary view refresh</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15327: fail to compile generated code with complex data structure</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.data source table created in InMemoryCatalog should be able to read/write</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15752 optimize metadata only query for datasource table</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16975: Column-partition path starting '_' should be handled correctly</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16674: field names containing dots for both fields and partitioned fields</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-18053: ARRAY equality is broken</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.should be able to resolve a persistent view</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19059: read file based table whose name starts with underscore</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-23079: constraints should be inferred correctly with aliases</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-22356: overlapped columns between data and partition schema in data source tables</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-part-after-analyze.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-table-column.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.null-handling.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.order-by-nulls-ordering.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.tablesample-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.typeCoercion/native/decimalArithmeticOperations.sql</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyzing views is not supported</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - unsupported types and invalid columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.test table-level statistics for data source table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - result verification</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.column stats collection for null columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after truncate command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after set location command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after insert command for datasource table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after inserts</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after table truncation</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after alter table add partition</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 1</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 2</li></div><div><li>org.apache.spark.sql.UDFSuite.SPARK-8005 input_file_name</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.treeString is redacted</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.explain is redacted using SQLConf</li></div><div><li>org.apache.spark.sql.execution.GlobalTempViewSuite.CREATE GLOBAL TEMP VIEW USING</li></div><div><li>org.apache.spark.sql.execution.SameResultSuite.FileSourceScanExec: different orders of data filters and partition filters</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.execution.WholeStageCodegenSuite.Skip splitting consume function when parameter number exceeds JVM limit</li></div><div><li>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.SPARK-22673: InMemoryRelation should utilize existing stats of the plan to be cached</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Hive Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-23348: append data to data source table with saveAsTable</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.empty file should be skipped while write to file</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.SPARK-22252: FileFormatWriter should respect the input query schema</li></div><div><li>org.apache.spark.sql.execution.datasources.FileIndexSuite.SPARK-20367 - properly unescape column names in inferPartitioning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] partition pruned file scans implement sameResult correctly</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] exchange reuse respects differences in partition pruning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-18753] keep pushed-down null literal as a filter in Spark-side post-filter</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.sizeInBytes should be the total size of all files</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.SPARK-22790: spark.sql.sources.compressionFactor takes effect</li></div><div><li>org.apache.spark.sql.execution.datasources.json.JsonSuite.SPARK-7565 MapType in JsonRDD</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite.Create parquet table with compression</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Dictionary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Null</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.Read row group containing both dictionary and plain encoded pages</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.read parquet footers in parallel</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.basic data types (without binary)</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.raw binary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.string</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.date type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nulls</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nones</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.compression codec</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - overwrite</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - ignore</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - throw</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - append</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-8121: spark.sql.parquet.output.committer.class shouldn't be overridden</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-7837 Do not close output writer twice when commitTask() fails</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-11044 Parquet writer version fixed as version1</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.null and non-null strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.read dictionary and plain encoded timestamp_millis written as INT64</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-12589 copy() on rows returned from reader works for strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - direct path read</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - partition column types</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.parquet timestamp conversion</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema merging failure error message</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.Read Parquet file generated by parquet-thrift</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.CartesianProduct metrics</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.SortMergeJoin(left-anti) metrics</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.range metrics</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics: parquet</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics with dynamic partition: parquet</li></div><div><li>org.apache.spark.sql.execution.streaming.RateSourceV2Suite.basic microbatch execution</li></div><div><li>org.apache.spark.sql.internal.CatalogSuite.dropTempView should not un-cache and drop metastore table if a same-name table exists</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with partitioned by</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with valid number of buckets</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.SPARK-17409: CTAS of decimal calculation</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions with repeats</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.partitioned columns should appear at the end of schema</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in non-partitioned write path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.append data to an existing partitioned table without custom partition path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.timeZone setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.streaming.DeduplicateSuite.deduplicate with file sink</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.unpartitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-21167: encode and decode path correctly</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading with 'basePath'</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.writing with aggregation</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, no schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files with changing schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - unpartitioned output</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - partitioned output</li></div><div><li>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.write path implements onTaskCommit API correctly</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.parquet - API and behavior regarding schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.column nullability and comment - write and then read</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-17230: write out results of decimal calculation</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18899: append to a bucketed table using DataFrameWriter with mismatched bucketing</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18913: append to a table with special column names</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-20460 Check name duplication in schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.use Spark jobs to list files</li></div><div><li>org.apache.spark.sql.util.DataFrameCallbackSuite.execute callback functions for DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.REFRESH TABLE also needs to recache the data (data source tables)</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-15678: REFRESH PATH</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.Cache/Uncache Qualified Tables</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-11246 cache parquet table</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.both table-level and session-level compression are set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.table-level compression is not set but session-level compressions is set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.test table containing mixed compression codec</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Case insensitive attribute names</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.SELECT on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Simple column projection + filter on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Converting Hive to Parquet Table via saveAsParquetFile</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.INSERT OVERWRITE TABLE Parquet table</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT OVERWRITE - partition IF NOT EXISTS</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - SPARK-16037: INSERT statement should match columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT INTO a partitioned table (semantic and error handling)</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match columns by position and ignore column names</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match unnamed columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.SPARK-21165: FileFormatWriter should only rely on attributes from analyzed plan</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.convert partition provider to hive with repair table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is enabled, new tables have partition provider hive</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, new tables have no partition provider</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, we preserve the old behavior even for new tables</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of legacy datasource table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of new datasource table overwrites just partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.append data with DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19359: renaming partition should not leave useless directories</li></div><div><li>org.apache.spark.sql.hive.ShowCreateTableSuite.data source table using Dataset API</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.size estimation for relations is based on row size * number of rows</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.analyze non hive compatible datasource tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from Hive serde tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify serialized column stats after analyzing columns</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify column stats can be deserialized from tblproperties</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.serialization and deserialization of histograms to/from hive metastore</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for data source table created in HiveExternalCatalog</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for partitioned data source table</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external tables in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external data source table in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-19129: drop partition with a empty string will drop the whole table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop views</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - rename</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - set/unset tblproperties</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views and alter table - misuse</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop view using drop table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a temporary view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE an external data source table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.desc table for data source table - no user-defined schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with Catalog</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with DataFrameWriter.saveAsTable</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.append data to hive serde table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partitioned table should always put partition columns at the end of table schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- partitioned - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- with predicate - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-22252: FileFormatWriter should respect the input query schema in HIVE</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a hive, built-in, and permanent user function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a temporary function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.SPARK-14933 - create view from hive parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-6851: Self-joined converted parquet tables</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde without location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with default fileformat</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde with location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with serde</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-10741: Sort on Aggregate using parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - hive</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-11453: append data to partitioned table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.insert into datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-15752 optimize metadata only query for hive table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-19912 String literals should be escaped for Hive metastore partition pruning</li></div><div><li>org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div></ol></td><td><ol><div><li>org.apache.spark.JobCancellationSuite.interruptible iterator of shuffle reader</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.CartesianProduct metrics</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol><div><li>&amp;#010;Error while stopping stream: &amp;#010;query.exception() is not empty after clean stop: org.apache.spark.sql.streaming.StreamingQueryException: Writing job failed.&amp;#010;=== Streaming Query ===&amp;#010;Identifier: [id = 3cdd9824-61ff-4b93-94b9-88a1e4297394, runId = ff6adb81-75c5-4ab7-8cad-3bdddd681e8c]&amp;#010;Current Committed Offsets: {org.apache.spark.sql.execution.streaming.continuous.RateStreamCon</li></div><div><li>&amp;#010;Error while stopping stream: &amp;#010;query.exception() is not empty after clean stop: org.apache.spark.sql.streaming.StreamingQueryException: Writing job failed.&amp;#010;=== Streaming Query ===&amp;#010;Identifier: [id = 257bf80e-9923-4638-80cb-feebfd2a20bf, runId = 5643b02e-6ed5-4b3f-bf8f-f1cc79e4af32]&amp;#010;Current Committed Offsets: {org.apache.spark.sql.execution.streaming.continuous.RateStreamCon</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-1265833954850958383.8: libsnappy.so.1: cannot open shared object file: No such file or directory]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy(UnsafeShuffleWriterSuite.java:364)
Cau</li></div><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy(UnsafeShuffleWriterSuite.java:359)
Cau</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;INSERT INTO TABLE t PARTITION (ds='2017-08-01', hr=10)&amp;#010;VALUES ('k1', 100), ('k2', 200), ('k3', 300)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #17&amp;#010;INSERT INTO desc_col_table values 1, 2, 3, 4</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;insert into t1 values(1,0,0)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;INSERT INTO spark_10747 VALUES (6, 12, 10), (6, 11, 4), (6, 9, 10), (6, 15, 8),&amp;#010;(6, 15, 8), (6, 7, 4), (6, 7, 8), (6, 13, null), (6, 10, null)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #5&amp;#010;insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),&amp;#010;  (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>"[Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 59, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)&amp;#010</li></div><div><li>"Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 60, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)&amp;#010;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 98, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompre</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompresso</li></div><div><li>2 did not equal 1</li></div><div><li>2 did not equal 1</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>&amp;#010;&amp;#010;== Results ==&amp;#010;!== Correct Answer - 10 ==     == Spark Answer - 0 ==&amp;#010;!struct&lt;_1:timestamp,_2:int&gt;   struct&lt;&gt;&amp;#010;![1969-12-31 16:00:00.0,0]     &amp;#010;![1969-12-31 16:00:00.1,1]     &amp;#010;![1969-12-31 16:00:00.2,2]     &amp;#010;![1969-12-31 16:00:00.3,3]     &amp;#010;![1969-12-31 16:00:00.4,4]     &amp;#010;![1969-12-31 16:00:00.5,5]     &amp;#010;![1969-12-31 16:00:00.6,6]     &amp;#010;![1969</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Query [id = 4887c463-f952-4a15-b1ad-cc10f13218b2, runId = 3e7c4f9a-838f-46b1-905a-fd7ec9236d21] terminated with exception: Job aborted.</li></div><div><li>Query [id = 572f8b5d-0509-4ba3-bfe0-23b0cb4706c3, runId = 72f99d5b-50c4-44dd-9b3f-ffc57accbc96] terminated with exception: Job aborted.</li></div><div><li>Query [id = f5abf917-3706-4b4c-b0e0-e82e8227c1a7, runId = 13aea4ef-e495-43bb-8894-b136eba71b1a] terminated with exception: Job aborted.</li></div><div><li>Query [id = b31489c9-2713-4c01-91da-929c7fecafc8, runId = 4f3bf723-8227-4f60-b8dc-d6904ae4e2aa] terminated with exception: Job aborted.</li></div><div><li>Query [id = 3ac503d4-df2b-45d6-bf7b-b02a8d9a3204, runId = f79cde5f-20bd-44cc-b2fb-ba0f7d00eba5] terminated with exception: Job aborted.</li></div><div><li>Query [id = 94957514-4374-4857-9382-4e31a1deb4c1, runId = 3f4d1ab3-0897-4e31-b708-3e21ec46320f] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>&amp;#010;Error adding data: Job aborted.&amp;#010;org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)&amp;#010; org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)&amp;#010; org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)&amp;#010; org.apac</li></div><div><li>Job aborted.</li></div><div><li>Query [id = fc87338f-567a-490f-8cbd-f62776dbc028, runId = 0518e042-1293-4ac0-967c-6be200467a2a] terminated with exception: Job aborted.</li></div><div><li>Query [id = be48cc6f-cdc3-4f18-9813-c0f6849924a1, runId = 6b7ee458-0b94-4c20-a246-7d2c835777cc] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div></ol></td><td><ol><div><li>122 was not less than or equal to 10</li></div><div><li>2 did not equal 1</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.restarts</div></li><li><div>org.apache.spark.sql.streaming.continuous.ContinuousSuite.repeatedly restart</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li><li><div>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy</div></li><li><div>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.sql.CachedTableSuite.SPARK-19765: UNCACHE TABLE should un-cache all cached plans that refer to this table</div></li><li><div>org.apache.spark.sql.CachedTableSuite.refreshByPath should refresh all cached plans with the specified path</div></li><li><div>org.apache.spark.sql.DataFrameJoinSuite.broadcast join hint using broadcast function</div></li><li><div>org.apache.spark.sql.DataFrameSuite.inputFiles</div></li><li><div>org.apache.spark.sql.DataFrameSuite.SPARK-6941: Better error message for inserting into RDD-based Table</div></li><li><div>org.apache.spark.sql.DataFrameSuite.SPARK-11301: fix case sensitivity for filter on partitioned columns</div></li><li><div>org.apache.spark.sql.DataFrameSuite.fix case sensitivity of partition by</div></li><li><div>org.apache.spark.sql.DatasetSuite.SPARK-22472: add null check for top-level primitive values</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-23072 Write and read back unicode column names - parquet</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-22146 read files containing special characters using parquet</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.Enabling/disabling ignoreMissingFiles using parquet</div></li><li><div>org.apache.spark.sql.MathFunctionsSuite.round/bround with table columns</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.SPARK-16337 temporary view refresh</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.case sensitivity support in temporary view refresh</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.specifying database name for a temporary view is not allowed</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-15327: fail to compile generated code with complex data structure</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.data source table created in InMemoryCatalog should be able to read/write</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-15752 optimize metadata only query for datasource table</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-16975: Column-partition path starting '_' should be handled correctly</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-16674: field names containing dots for both fields and partitioned fields</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-18053: ARRAY equality is broken</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.should be able to resolve a persistent view</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-19059: read file based table whose name starts with underscore</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-23079: constraints should be inferred correctly with aliases</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-22356: overlapped columns between data and partition schema in data source tables</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.columnresolution-negative.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.columnresolution.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.describe-part-after-analyze.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.describe-table-column.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.null-handling.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.order-by-nulls-ordering.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.tablesample-negative.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.typeCoercion/native/decimalArithmeticOperations.sql</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.SPARK-18856: non-empty partitioned table should not report zero size</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyzing views is not supported</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - unsupported types and invalid columns</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.test table-level statistics for data source table</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - result verification</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.column stats collection for null columns</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after truncate command</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after set location command</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after insert command for datasource table</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after inserts</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after table truncation</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after alter table add partition</div></li><li><div>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 1</div></li><li><div>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 2</div></li><li><div>org.apache.spark.sql.UDFSuite.SPARK-8005 input_file_name</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - Repartition UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - Repartition UDTs with Parquet</div></li><li><div>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.treeString is redacted</div></li><li><div>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.explain is redacted using SQLConf</div></li><li><div>org.apache.spark.sql.execution.GlobalTempViewSuite.CREATE GLOBAL TEMP VIEW USING</div></li><li><div>org.apache.spark.sql.execution.SameResultSuite.FileSourceScanExec: different orders of data filters and partition filters</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.create view for partitioned parquet table</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.Using view after adding more columns</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.correctly handle a view with custom column names</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.sparkSession API view resolution with different default database</div></li><li><div>org.apache.spark.sql.execution.WholeStageCodegenSuite.Skip splitting consume function when parameter number exceeds JVM limit</div></li><li><div>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.SPARK-22673: InMemoryRelation should utilize existing stats of the plan to be cached</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter table: rename cached table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Data Source Table As Select</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create temporary view with mismatched schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SET LOCATION for managed table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create datasource table with a non-existing location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a non-existing location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a existed location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a:b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a%b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a,b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - parquet</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Hive Table As Select</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-23348: append data to data source table with saveAsTable</div></li><li><div>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.empty file should be skipped while write to file</div></li><li><div>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.SPARK-22252: FileFormatWriter should respect the input query schema</div></li><li><div>org.apache.spark.sql.execution.datasources.FileIndexSuite.SPARK-20367 - properly unescape column names in inferPartitioning</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] partition pruned file scans implement sameResult correctly</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] exchange reuse respects differences in partition pruning</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-18753] keep pushed-down null literal as a filter in Spark-side post-filter</div></li><li><div>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.sizeInBytes should be the total size of all files</div></li><li><div>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.SPARK-22790: spark.sql.sources.compressionFactor takes effect</div></li><li><div>org.apache.spark.sql.execution.datasources.json.JsonSuite.SPARK-7565 MapType in JsonRDD</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite.Create parquet table with compression</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Dictionary</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Null</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.Read row group containing both dictionary and plain encoded pages</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.read parquet footers in parallel</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.basic data types (without binary)</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.raw binary</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.string</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - fixed-length decimals</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - fixed-length decimals</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.date type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - map</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - map</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array and double</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array and double</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - struct</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - struct</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested struct with array of array as field</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested struct with array of array as field</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested map with struct as value type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested map with struct as value type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nulls</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nones</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.compression codec</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - overwrite</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - ignore</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - throw</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - append</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-8121: spark.sql.parquet.output.committer.class shouldn't be overridden</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-7837 Do not close output writer twice when commitTask() fails</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-11044 Parquet writer version fixed as version1</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.null and non-null strings</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.read dictionary and plain encoded timestamp_millis written as INT64</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-12589 copy() on rows returned from reader works for strings</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - direct path read</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - partition column types</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.parquet timestamp conversion</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema merging failure error message</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.Read Parquet file generated by parquet-thrift</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.SortMergeJoin(left-anti) metrics</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.range metrics</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics: parquet</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics with dynamic partition: parquet</div></li><li><div>org.apache.spark.sql.execution.streaming.RateSourceV2Suite.basic microbatch execution</div></li><li><div>org.apache.spark.sql.internal.CatalogSuite.dropTempView should not un-cache and drop metastore table if a same-name table exists</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.read bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when join 2 bucketed tables</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join keys are not equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.error if there exists any malformed bucket files</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with partitioned by</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with valid number of buckets</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.SPARK-17409: CTAS of decimal calculation</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions with repeats</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.partitioned columns should appear at the end of schema</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in non-partitioned write path</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in dynamic partition writes</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.append data to an existing partitioned table without custom partition path</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.timeZone setting in dynamic partition writes</div></li><li><div>org.apache.spark.sql.streaming.DeduplicateSuite.deduplicate with file sink</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.unpartitioned writing and batch reading</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-21167: encode and decode path correctly</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading with 'basePath'</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.writing with aggregation</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, no schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files with changing schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - unpartitioned output</div></li><li><div>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - partitioned output</div></li><li><div>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.write path implements onTaskCommit API correctly</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.parquet - API and behavior regarding schema</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.column nullability and comment - write and then read</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-17230: write out results of decimal calculation</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18899: append to a bucketed table using DataFrameWriter with mismatched bucketing</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18913: append to a table with special column names</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-20460 Check name duplication in schema</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.use Spark jobs to list files</div></li><li><div>org.apache.spark.sql.util.DataFrameCallbackSuite.execute callback functions for DataFrameWriter</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.REFRESH TABLE also needs to recache the data (data source tables)</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.SPARK-15678: REFRESH PATH</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.Cache/Uncache Qualified Tables</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.SPARK-11246 cache parquet table</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.both table-level and session-level compression are set</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.table-level compression is not set but session-level compressions is set</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.test table containing mixed compression codec</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Case insensitive attribute names</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.SELECT on Parquet table</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Simple column projection + filter on Parquet table</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Converting Hive to Parquet Table via saveAsParquetFile</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.INSERT OVERWRITE TABLE Parquet table</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT OVERWRITE - partition IF NOT EXISTS</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - SPARK-16037: INSERT statement should match columns by position</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT INTO a partitioned table (semantic and error handling)</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match columns by position and ignore column names</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match unnamed columns by position</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.SPARK-21165: FileFormatWriter should only rely on attributes from analyzed plan</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.convert partition provider to hive with repair table</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is enabled, new tables have partition provider hive</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, new tables have no partition provider</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, we preserve the old behavior even for new tables</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of legacy datasource table</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of new datasource table overwrites just partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into partial dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into fully dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into static partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite partial dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite fully dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite static partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.append data with DataFrameWriter</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19359: renaming partition should not leave useless directories</div></li><li><div>org.apache.spark.sql.hive.ShowCreateTableSuite.data source table using Dataset API</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.SPARK-18856: non-empty partitioned table should not report zero size</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.size estimation for relations is based on row size * number of rows</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.analyze non hive compatible datasource tables</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from Hive serde tables</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.verify serialized column stats after analyzing columns</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.verify column stats can be deserialized from tblproperties</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.serialization and deserialization of histograms to/from hive metastore</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for data source table created in HiveExternalCatalog</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for partitioned data source table</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter table: rename cached table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create Data Source Table As Select</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create temporary view with mismatched schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SET LOCATION for managed table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create datasource table with a non-existing location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a non-existing location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a existed location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a:b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a%b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a,b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external tables in default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external data source table in default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-19129: drop partition with a empty string will drop the whole table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop views</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - rename</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - set/unset tblproperties</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views and alter table - misuse</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop view using drop table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create view with mismatched schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a temporary view</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE an external data source table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a view</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.desc table for data source table - no user-defined schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with Catalog</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with DataFrameWriter.saveAsTable</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.append data to hive serde table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partitioned table should always put partition columns at the end of table schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a:b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a%b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a,b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- partitioned - PARQUET</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- with predicate - PARQUET</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-22252: FileFormatWriter should respect the input query schema in HIVE</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create view for partitioned parquet table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.Using view after adding more columns</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.correctly handle a view with custom column names</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.sparkSession API view resolution with different default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a hive, built-in, and permanent user function</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a temporary function</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.SPARK-14933 - create view from hive parquet table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-6851: Self-joined converted parquet tables</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde without location</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with default fileformat</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde with location</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with serde</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.specifying database name for a temporary view is not allowed</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-10741: Sort on Aggregate using parquet</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - hive</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-11453: append data to partitioned table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.insert into datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-15752 optimize metadata only query for hive table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-19912 String literals should be escaped for Hive metastore partition pruning</div></li><li><div>org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.read bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when join 2 bucketed tables</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join keys are not equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.error if there exists any malformed bucket files</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.spark.JobCancellationSuite.interruptible iterator of shuffle reader</div></li></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="sqoop"><div class="panel-heading">SQOOP</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/trunk</div><div>Last Revision: a7f5e0d298ffbf8e674bd35ee10f2accc1da5453</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="storm"><div class="panel-heading">STORM</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: ffa607e2464a361a8f2fa548cc8043f5a8818d04</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1173</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1163</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1167</div><div>Failed Count : 5</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1173</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</li></div><div><li>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithKeyAndKeyValueScheme</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol><div><li>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</li></div><div><li>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties</li></div><div><li>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithKeyAndKeyValueScheme</li></div><div><li>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithoutKeyAndKeyValueScheme</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>
Wanted but not invoked:
collector.ack(
    source: null:1, stream: , id: {}, [value-234] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null
);
-&gt; at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:260)

However, there were exactly 2 interactions with this mock:
collector.reportError(
    java.util.concurrent.ExecutionException: org.apache.kafka.common.err</li></div><div><li>org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.</li></div><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol><div><li>
Wanted but not invoked:
collector.ack(
    source: null:1, stream: , id: {}, [value-234] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null
);
-&gt; at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:260)

However, there were exactly 2 interactions with this mock:
collector.reportError(
    java.util.concurrent.ExecutionException: org.apache.kafka.common.err</li></div><div><li>
Wanted but not invoked:
collector.ack(
    source: null:1, stream: , id: {}, [[B@5a2bd7c8, [B@7ca8d498] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null
);
-&gt; at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:202)

However, there were exactly 2 interactions with this mock:
collector.reportError(
    java.util.concurrent.ExecutionExcept</li></div><div><li>org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.</li></div><div><li>org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.</li></div><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol><div><li>Cannot run program "node": error=2, No such file or directory</li></div><div><li>Unable to send halt interrupt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties</div></li><li><div>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithoutKeyAndKeyValueScheme</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="tez"><div class="panel-heading">TEZ</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 82d73b380881ef8e7d6e6c963289c4f479bbea59</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1765</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1668</div><div>Failed Count : 17</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1765</div><div>Failed Count : 5</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1765</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandlerJobs.testOrderedWordCount</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDisableSessionLogging</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testSimpleAMACls</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDAGACls</li></div><div><li>org.apache.tez.dag.history.logging.ats.TestATSHistoryWithMiniCluster.testDisabledACls</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJob</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJobWithCompression</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testFailingAttempt</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testFailingJob</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testRandomWriter</li></div><div><li>org.apache.tez.test.TestDAGRecovery.testBasicRecovery</li></div><div><li>org.apache.tez.test.TestDAGRecovery.testDelayedInit</li></div><div><li>org.apache.tez.test.TestDAGRecovery2.testSessionDisableMultiAttempts</li></div><div><li>org.apache.tez.test.TestDAGRecovery2.testFailingCommitter</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationNonSession</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.org.apache.tez.analyzer.TestAnalyzer</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.app.dag.impl.TestCommit.testDAGCommitFail3_OnVertexSuccess</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession(TestExceptionPropagation.java:234)
</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>java.io.IOException: ApplicationHistoryServer failed to start. Final state is STOPPED</li></div></ol></td><td><ol><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8463763887342093313.8: libsnappy.so.1: cannot open shared object file: No such file or directory]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Application not running, applicationId=application_1520483116835_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[Session stats:submittedDAGs=4, successfulDAGs=0, failedDAGs=4, killedDAGs=0]</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1520482643618_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=6, successfulDAGs=0, failedDAGs=7, killedDAGs=0]</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.tez.auxservices.TestShuffleHandlerJobs.testOrderedWordCount</div></li><li><div>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDisableSessionLogging</div></li><li><div>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testSimpleAMACls</div></li><li><div>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDAGACls</div></li><li><div>org.apache.tez.dag.history.logging.ats.TestATSHistoryWithMiniCluster.testDisabledACls</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJob</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJobWithCompression</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobs.testFailingAttempt</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobs.testFailingJob</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobs.testRandomWriter</div></li><li><div>org.apache.tez.test.TestDAGRecovery.testBasicRecovery</div></li><li><div>org.apache.tez.test.TestDAGRecovery.testDelayedInit</div></li><li><div>org.apache.tez.test.TestDAGRecovery2.testSessionDisableMultiAttempts</div></li><li><div>org.apache.tez.test.TestDAGRecovery2.testFailingCommitter</div></li><li><div>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationNonSession</div></li><li><div>org.apache.tez.analyzer.TestAnalyzer.org.apache.tez.analyzer.TestAnalyzer</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.tez.dag.app.dag.impl.TestCommit.testDAGCommitFail3_OnVertexSuccess</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="zeppelin"><div class="panel-heading">ZEPPELIN</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 0267ecf76d313ba3c3ccea2d35e9b1919c7c9956</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 659</div><div>Failed Count : 33</div><div>Skipped Count : 0</div></td><td><div>Total Count : 646</div><div>Failed Count : 30</div><div>Skipped Count : 0</div></td><td><div>Total Count : 647</div><div>Failed Count : 37</div><div>Skipped Count : 0</div></td><td><div>Total Count : 643</div><div>Failed Count : 21</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.configuration.RequestHeaderSizeTest.increased_request_header_size_do_not_cause_413_when_request_size_is_over_8K</li></div><div><li>org.apache.zeppelin.recovery.RecoveryTest.org.apache.zeppelin.recovery.RecoveryTest</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.HeliumRestApiTest.org.apache.zeppelin.rest.HeliumRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.socket.NotebookServerTest.org.apache.zeppelin.socket.NotebookServerTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.HeliumRestApiTest.org.apache.zeppelin.rest.HeliumRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--4c0f5442b839.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
03:30:03,392  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--9ae8d211cb7c.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
03:49:02,751  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion(PySparkInterpreterTest.java:142)
</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>Failed to create directory within 10000 attempts (tried 1520566352506-0 to 1520566352506-9999)</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.interpreter.AbstractInterpreterTest.tearDown(AbstractInterpreterTest.java:68)
	at org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.tearDown(FileSystemRecoveryStorageTest.java:41)
</li></div><div><li>Failed to create directory within 10000 attempts (tried 1520566352598-0 to 1520566352598-9999)</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.interpreter.AbstractInterpreterTest.tearDown(AbstractInterpreterTest.java:68)
	at org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.tearDown(FileSystemRecoveryStorageTest.java:41)
</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zeppelin.configuration.RequestHeaderSizeTest.increased_request_header_size_do_not_cause_413_when_request_size_is_over_8K</div></li><li><div>org.apache.zeppelin.recovery.RecoveryTest.org.apache.zeppelin.recovery.RecoveryTest</div></li><li><div>org.apache.zeppelin.socket.NotebookServerTest.org.apache.zeppelin.socket.NotebookServerTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion</div></li><li><div>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="display:none;" class="panel panel-default" name="data" id="zookeeper"><div class="panel-heading">ZOOKEEPER</div><div class="panel-body"><div>Branch Details: refs/remotes/origin/master</div><div>Last Revision: 99c9bbb0ab1eef469e1662086532c58078b9909a</div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU 16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1129</div><div>Failed Count : 2</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1130</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1130</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1127</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>org.apache.zookeeper.server.SessionTrackerTest.testAddSessionAfterSessionExpiry</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>Should throw session expiry exception as the session has expired and closed</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</div></li><li><div>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zookeeper.server.SessionTrackerTest.testAddSessionAfterSessionExpiry</div></li><li><div>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</div></li><li><div>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</div></li><li><div>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</div></li></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;display:block" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPCUBUNTU16 SUMMARY</div></div><table class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th>Failed Count</th><th>Unique Count</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img>ABORTED</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>4</td><td>4</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>13</td><td>2</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>2</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>33</td><td>32</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>20</td><td>14</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>1</td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>24</td><td>0</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>0</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>0</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>33</td><td>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>2</td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86UBUNTU16 SUMMARY</div></div><table class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th>Failed Count</th><th>Unique Count</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>1</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>4</td><td>3</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>2</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>1</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>12</td><td>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>1</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>1</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>6</td><td>0</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>24</td><td>0</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>3</td><td>2</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>3</td><td>0</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>17</td><td>16</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>30</td><td>0</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPCRHEL7 SUMMARY</div></div><table class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th>Failed Count</th><th>Unique Count</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>0</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>14</td><td>13</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>16</td><td>4</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>0</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>3</td><td>3</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img>ABORTED</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>117</td><td>117</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>3</td><td>3</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>441</td><td>440</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>5</td><td>2</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>5</td><td>3</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>37</td><td>6</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86RHEL7 SUMMARY</div></div><table class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th>Failed Count</th><th>Unique Count</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>12</td><td>0</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>0</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>4</td><td>2</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>1</td><td>1</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>3</td><td>1</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td></td><td></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>1</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>2</td><td>0</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>21</td><td>0</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>4</td><td>4</td></tr></tbody></table></div><div id="ppcx86" style="display:none;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img>0 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>14 (13)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>16 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (0)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>13 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/aborted.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>117 (117)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (32)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (14)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>6 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>441 (440)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>17 (16)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>30 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>37 (6)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>21 (0)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td></tr></tbody></table></div></div></body></html>